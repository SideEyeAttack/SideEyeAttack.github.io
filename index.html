<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>


<head>
    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">google.load("jquery", "1.3.2");</script>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">
      <!-- Custom styles for this template -->
        
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>
    <!-- <link rel="icon" href="img/lightcommands.png"> -->
</head>
<style type="text/css">
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 17px;
        margin-left: auto;
        margin-right: auto;
        width: 70%;
    }

    h1 {
        font-weight: 300;
        line-height: 1.15em;
    }

    h2 {
        font-size: 2em;
    }

    a:link, a:visited {
        color: #00aeff;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }


    b:link, b:visited {
        color: #00aeff;
        text-decoration: none;
    }

    b:hover {
        color: #208799;
    }

    h1, h2, h3 {
        text-align: center;
    }

    h1 {
        font-size: 40px;
        font-weight: 500;
    }

    h2 {
        font-weight: 400;
        margin: 16px 0px 4px 0px;
    }

    .paper-title {
        padding: 16px 0px 16px 0px;
    }

    section {
        margin: 32px 0px 32px 0px;
        text-align: justify;
        clear: both;
    }

    .col-5 {
        width: 20%;
        float: left;
    }

    .col-4 {
        width: 25%;
        float: left;
    }

    .col-3 {
        width: 33%;
        float: left;
    }

    .col-2 {
        width: 50%;
        float: left;
    }


    .fig-center {
        width: 80%;
        float: center;
    }

    .col-1 {
        width: 100%;
        float: left;
    }

    .row, .author-row, .affil-row {
        overflow: auto;
    }

    .author-row, .affil-row {
        font-size: 26px;
    }

    .row {
        margin: 16px 0px 16px 0px;
    }

    .authors {
        font-size: 26px;
    }

    .affil-row {
        margin-top: 16px;
    }

    .teaser {
        max-width: 100%;
    }

    .text-center {
        text-align: center;
    }

    .fig-caption {
        text-align: justify;
        max-width: 100%;

    }

    .audio-caption {
        text-align: center;
        max-width: 100%;

    }

    .screenshot {
        width: 80%;
        border: 1px solid #ddd;
    }

    .screenshot-el {
        margin-bottom: 1px;
    }

    hr {
        height: 1px;
        border: 0;
        border-top: 1px solid #ddd;
        margin: 0;
    }

    .material-icons {
        vertical-align: -6px;
    }

    p {
        line-height: 1.25em;
    }

    .caption {
        font-size: 16px;
        /*font-style: italic;*/
        color: #666;
        text-align: center;
        margin-top: 4px;
        margin-bottom: 10px;
    }

    video {
        display: block;
        margin: auto;
    }

    figure {
        display: block;
        margin: auto;
        margin-top: 10px;
        margin-bottom: 10px;
    }

    #bibtex pre {
        font-size: 13.5px;
        background-color: #eee;
        padding: 16px;
    }

    .blue {
        color: #2c82c9;
        font-weight: bold;
    }

    .orange {
        color: #d35400;
        font-weight: bold;
    }

    .overlayText {
        width: 80%;
        margin-left: auto;
        margin-right: auto;
    }

    .overlayTextFullWide {
        width: 100%;
        margin-left: auto;
        margin-right: auto;
    }

    .overlayTextHalfWide {
        width: 50%;
        margin-left: auto;
        margin-right: auto;
    }

    .flex-row {
        display: flex;
        flex-flow: row wrap;
        justify-content: space-around;
        padding: 0;
        margin: 0;
        list-style: none;
    }
    .table {
	width:100%;
	border:1px solid color-form-highlight;
    }
    
    .table-header {
    	display:flex;
    	width:100%;
    	background:rgb(32, 126, 181);
    	padding:(half-spacing-unit * 1.5) 0;
    }
    
    .table-row {
    	display:flex;
    	width:100%;
    	padding:(half-spacing-unit * 1.5) 0;
    
    }
    
    .table-data, .header__item {
    	flex: 1 1 20%;
    	text-align:center;
    }

    .line{
        border-left-width: 1px;
        border-left-style: solid;
        border-left-color: rgb(92, 84, 84);
    }

    .stack{
        display: block !important;
        width: auto;
    }
    
    .header__item {
    	text-transform:uppercase;
    }

    .paper-btn {
        position: relative;
        text-align: center;

        display: inline-block;
        margin: 8px;
        padding: 8px 8px;

        border-width: 0;
        outline: none;
        border-radius: 2px;

        background-color: #48b64e;
        color: white !important;
        font-size: 20px;
        width: 100px;
        font-weight: 600;
    }

    .paper-btn-parent {
        display: flex;
        justify-content: center;
        margin: 16px 0px;
    }

    .paper-btn:hover {
        opacity: 0.85;
    }

    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
    }
    .boxed {
				padding: 0.5em 2em 2em 2em;
				background-color: #F8F8F8;
	
				max-width: 90%;
				margin: 0 auto !important; 
				float: none !important; ;
	}
	.boxed_mini {
				padding: 0.5em 0.5em 0.5em 0.5em;
				max-width: 40%;
				background-color: #ECF9FF;
				
	}

    .venue {
        /*color: #B6486F;*/
        font-size: 30px;

    }

    .myButton_l {
				display:inline-block;
				cursor:pointer;
				font-family: Montserrat,sans-serif;
				font-weight: bold;
				font-size:15px;
				letter-spacing: 0.1em;
				padding:10px 10px;
				text-decoration:none;
			}
	.myButton {
				 background-color:#87d4f5;
				-moz-border-radius:18px;
				-webkit-border-radius:18px;
				border-radius:18px;
				display:inline-block;
				cursor:pointer;
				color:#ffffff;
				font-family: Montserrat,sans-serif;
				font-weight: bold;
				font-size:25px;
				letter-spacing: 0.1em;
				padding:10px 50px;
				text-decoration:none;
			}
	.myButton:hover {
				background-color:#478fcc;
				color:#ffffff;
				
			} 
            /* Style the navigation menu */
    .navbar {
    /* margin: 0%;
    height: 1%;
    padding: 0%; */
    margin: auto;
    text-align: center;
    padding-top: 0.14%;
    min-height: 25px;
    width: 100%;
    background-color: rgb(0, 140, 255);
    /*text-align: center;*/
    overflow: auto;
    }

    /* Navigation links */
    .navbar a {
    float: left;
    /*height: 90%;*/
    vertical-align: middle;
    color: rgb(255, 255, 255);
    text-decoration: none;
    font-size: 0.8em;
    width: 20%; /* Four equal-width links. If you have two links, use 50%, and 33.33% for three links, etc.. */
    text-align: center; /* If you want the text to be centered */
    }

    /* Add a background color on mouse-over */
    .navbar a:hover {
    background-color: rgb(0, 110, 255);
    }

    /* Style the current/active link */
    .navbar a.active {
    background-color:rgb(0, 120, 255);
    }

    /* Add responsiveness - on screens less than 500px, make the navigation links appear on top of each other, instead of next to each other */
    @media screen and (max-width: 500px) {
    .navbar a {
        float: none;
        display: block;
        width: 100%;
        text-align: left; /* If you want the text to be left-aligned on small screens */
    }
    }

</style>

<!-- End : Google Analytics Code-->
<script type="text/javascript" src="../js/hidebib.js"></script>
<!-- <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
      rel='stylesheet' type='text/css'> -->
<head>
    <title>Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses</title>
    <meta property="og:description" content="Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

</head>

<body>
<div class="flex-row">
    <div class="paper-title">
        <h1 style="color:rgb(71, 132, 216)"><strong>Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lensess</strong></h1>
    </div>

    <div id="authors">
        <center>
            <div class="author-row">
                <div class="col-3 text-center"><span style="font-size:25px">Yan Long</span><sup>2</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px">Pirouz Naghavi</span><sup>1</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px"></span>Blas Kojusner<sup>1</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px">Kevin Butler</span><sup>1</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px">Sara Rampazzi</span><sup>1</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px">Kevin Fu</span><sup>2</sup></div>
            </div>
            <br>
            <div class="text-left mt-auto" style="width:100%;margin-top: 1.5em; margin-bottom: 1.5em; display:flex;justify-content:space-around;align-items:center; flex-wrap: wrap;">
                <div>
                <a href="https://www.eng.ufl.edu/">
                  <img width = "320px" src="./img/uflogo.PNG" alt="University of Florida logo" class="img-fluid" />
              <div style = "height:10px"></div>
                </a>
                </div>
                <div>
                    <a href="https://www.cse.umich.edu">
                      <img width = "350px" src="./img/University-of-Michigan-Logo.png" alt="University of Michigan logo" class="img-fluid" />
                  <div style = "height:10px"></div>
                    </a>
                    </div>
                </div>
            <!-- <center>
                <br><br>
                <table align=center width=800px>
                    <tr>
                        <td align=center width=300px>
                            <center>
                                <span style="font-size:15px"><sup>1</sup> University of Florida</span>
                            </center>

                        </td>
                        <td align=center width=300px>
                            <center>
                                <span style="font-size:15px"><sup>2</sup> University of Michigan</span>
                            </center>
                        </td>
                        <td align=center width=300px>
                            <center>
                                <span style="font-size:15px"><sup>3</sup> The University of Electro-Communications (Tokyo)</span>
                            </center>
                        </td>
                    </tr>
                </table>
            </center>

        </center> -->
        
    </div></div>
    <!-- The navigation menu -->
    <div class="navbar" style="height: 5px;">
        <a class="active" href="#Abstract">Abstract</a>
        <a href="#Recovered Signals">Recovered Signals</a>
        <a href="#Processing Pipeline and ML Setup">Processing Pipeline and ML Setup</a>
        <a href="#Experiments & Results">Experiments & Results</a>
        <a href="#Defense">Defense</a>
    </div>

    <section id="Abstract"/>
    <hr>
    <div class="flex-row">
        <p>
            <br>
            <strong>Abstarct:</strong> Our research discovers how the rolling shutter and movable lens structures widely found in smartphone cameras modulate structure-borne sounds onto camera images, creating a point-of-view (POV) optical-acoustic side channel for acoustic eavesdropping. The movement of smartphone camera hardware leaks acoustic information because images unwittingly modulate ambient sound as imperceptible distortions. Our experiments find that the side channel is further amplified by intrinsic behaviors of Complementary metal-oxide–semiconductor (CMOS) rolling shutters and movable lenses such as in Optical Image Stabilization (OIS) and Auto Focus (AF). Our paper characterizes the limits of acoustic information leakage caused by structure-borne sound that perturbs the POV of smartphone cameras. In contrast with traditional optical-acoustic eavesdropping on vibrating objects, this side channel requires no line of sight and no object within the camera's field of view (images of a ceiling suffice). Our experiments test the limits of this side channel with a novel signal processing pipeline that extracts and recognizes the leaked acoustic information. Our evaluation with 10 smartphones on a spoken digit dataset reports 80.66%, 91.28% and 99.67% accuracies on recognizing 10 spoken digits, 20 speakers, and 2 genders respectively. We further systematically discuss the possible defense strategies and implementations. By modeling, measuring, and demonstrating the limits of acoustic eavesdropping from smartphone camera image streams, our contributions explain the physics-based causality and possible ways to reduce the threat on current and future devices.
                    To appear in <a href="https://www.ieee-security.org/TC/SP2023/">IEEE Symposium on Security and Privacy 2023<br><br></a>
        </p>
    </div>

	<div> 
        <figure style="width: 100%">
        <center><img width="80%" src="./img/proj_setup.png"></center>
		<div class="overlayText">
                <br>
                <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 1</strong> Illustration of the POV optical-acoustic side channel when a camera is recording a ceiling or floor. Adversaries can eavesdrop structure-borne sounds by extracting acoustic signals from artifacts of lens movement and rolling shutter patterns in smartphone cameras that depend on POV rather than objects within the field of view.</font></p>
            </div>
       
    </figure>
    </div>			
	<center>			
	<a href="https://arxiv.org/pdf/2210.09482.pdf" class="myButton">Read the Paper</a>
						
			
			
		<a href="#bibtex"  class=" myButton " data-toggle="collapse" role="button"><span class="material-icons"> insert_comment </span>
	Cite <i class="fa fa-quote-right" aria-hidden="true"></i></a>
		<div id="bibtex" style="margin-top: 1.5em;" class="collapse" align="left">
			<pre style="white-space: pre">

    @inproceedings{cao2023youcantseeme,
    title={You Can’t See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks},
    author={Yulong Cao and S. Hrushikesh Bhupathiraju and Pirouz Naghavi and Takeshi Sugawara and Z. Morley Mao and Sara Rampazzi},
    booktitle={32nd {USENIX} Security Symposium ({USENIX} Security 23)},
    year={2023}
    }
    </center>
	</pre>					
	</div>	
    </section>

    <section id="Recovered Signals"/>
    <h2>Recovered Signals</h2>
    <hr>
    <div class="flex-row">
        <p>
            <br>Some of the original and side channel recovered signals are provided below. The original audio is from the <a href="https://github.com/soerenab/AudioMNIST">AudioMNIST dataset</a>. Recovered signals are recorded by the rear camera of Pixel 3 when a speaker is playing the original signals at 85 dB. Provided samples demonstrate the intelligibility of the recovered signals. Although the recovered audio is unintelligible in the sense that people cannot directly tell what exact words are spoken without any reference, it preserves clear low-frequency tones that allow listeners to tell the gender and even identity of the speaker, which already allows for a certain level of privacy leakage. this paper is centered on machine learning recognition in the hope to explore the limits of this side channel’s capacity. Listeners may also be able to differentiate between different words and sentences if they are given a closed set of possible candidates. Furthermore, the NIST-SNR and STOI intelligibility scores added in the paper show that our extracted signals have relatively good signal strength and some level of intelligibility compared to previous works’ signals.
        </p>
    </div>

    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/36.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText"style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Original audio of speaker 36 (Female)</p>
            </div>
    </div>
    
    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/36_recover.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Recovered audio of speaker 36 (Female)</p>
            </div>
    </div>

    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/56.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Original audio of speaker 56 (Female)</p>
            </div>
    </div>
    
    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/56_recover.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Recovered audio of speaker 56 (Female)</p>
            </div>
    </div>

    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/06.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText"style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Original audio of speaker 6 (Male)</p>
            </div>
    </div>
    
    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/06_recover.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Recovered audio of speaker 6 (Male)</p>
            </div>
    </div>

    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/09.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Original audio of speaker 59 (Male)</p>
            </div>
    </div>
    
    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/09_recover.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Recovered audio of speaker 9 (Male)</p>
            </div>
    </div>
	
    <br>
    
    <div style="margin-top: 40%;"> 
        <figure style="width: 100%">
        <center><img width="80%" src="./img/chirp.png"></center>
		<div class="overlayText">
                
                <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 2</strong> The recovered chirp signals (50-650 Hz in 7s) with different camera control parameters and a 30 fps frame rate. (a) Optimized parameters and 1 ms exposure time. (b) OIS is left on. (c) EIS is left on. (d) 10 ms exposure time. (e) OIS, EIS, AF are left on with 10 ms exposure time. (f) Recovered with the phone stock camera app without any optimization.
                </font></p>
            </div>
       
    </figure>
    </div>
    
    <div style="margin-top: 10%;"> 
        <figure style="width: 100%">
        <center><img width="80%" src="./img/ort.png"></center>
		<div class="overlayText">
            <br>
                <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 3</strong> The chirp signal recovered with different orientations of the phone with respect to the sound source (a) 0 degrees. (b) 45 degrees. (c) 90 degrees. We observe no significant variations for different orientation.
                </font></p>
            </div>
       
    </figure>
    </div>

    <div style="margin-top: 10%;"> 
        <figure style="width: 100%">
        <center><img width="80%" src="./img/digit.png"></center>
		<div class="overlayText">
            <br>
                <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 4</strong> The waveform and spectrogram of spoken digits "zero", "seven", and "nine". (a) The original signals. (b) The recovered signals from a 3.2s 30fps video with the optimized camera parameters.
                </font></p>
            </div>
       
    </figure>
    </div>
	</section>

    <section id="Processing Pipeline and ML Setup"/>
		<br>
        <h2>Processing Pipeline and ML Setup</h2>
        <hr>
        <div class="flex-row">
            <br>
            <p>
                <strong>Signal Processing Pipeline: </strong>As shown in Figure 5, our attack system is a signal processing pipeline that consists of the following three stages. (1) Signal extraction. The stage implements the extraction algorithm shows in Section IV-C. It accepts videos collected by the malicious app and outputs (8 in the case of 1080p videos) channels of 1D signals. (2) Pre-processing. It performs noise reduction, liveness detection, trimming, low-pass filtering, and normalization to the channels. As shown in Figure 2, the extracted signals contain non-trivial but spectrally-static noise caused by different imaging and image registration noise. We thus first apply a background noise reduction step using the spectral-subtraction noise removal method. We then conduct a channel-wise amplitude-based liveness detection that determines the start and end index of the contained speech signals. Afterwards, we average the start and end indices of the channels and trim them to remove the parts without speech signals. We further apply a digital low pass filter with a cutoff frequency of 4kHz to get rid of the remaining high-frequency disturbances caused by camera imaging noise. Finally, we normalize the channels and pass them to the next stage. (3) Classification. Our classification stage implements a classification model that builds upon the Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) large, which is introduced next.
            </p>
        </div>
        <div style="margin-top: 10%;"> 
            <figure style="width: 100%">
            <center><img width="100%" src="./img/pipeline_small.png"></center>
            <div class="overlayTextFullWide">
                <br>
                    <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 5</strong> Our signal processing pipeline that exploits the optical-acoustic side channel on smartphone cameras. The signal extraction stage extracts sound-induced signals from the videos recorded on smartphones. The pre-processing stage cleans up the signals and feeds them into the classification model, where the gender, speaker, and speech content are recognized.
                    </font></p>
                </div>
           
        </figure>
        </div>
        <div class="flex-row">
            <br>
            <p>
                <strong>Classification Model: </strong> <br><br>

                <strong>Transfer Learning. </strong>HuBERT is a machine learning model that implements the concept of transfer learning. Essentially, the acoustic signals we extract are the embeddings of the original speech signals in the lens motion domain. Although building classification models in such a new signal domain often requires a significant amount of data to sufficiently sample the target domain, the adversary can overcome this challenge by utilizing transfer learning. Simply put, transfer learning allows the model to leverage the knowledge gained in the source domain (speech audio) and apply it to the tasks in the target domain (lens motion). Specifically, the original HuBERT model was pre-trained on 60,000 hours of speech derived from the LibriVox project and formalized in Libri-light, as well as 960 hours of speech from Librispeech. This large amount of incorporated knowledge enables the model to rapidly adapt to the lens motion distribution.
                    
                <br><br>
                    
                <strong>Waveform Input. </strong>HuBERT uses raw waveform as inputs. Compared to other popular types of speech recognition inputs, such as mel-frequency cepstral coefficients (MFCCs) and signal spectrogram, raw waveform inputs preserve the largest amount of usable information. Raw audio waveform inputs have also been demonstrated in speech recognition research to achieve higher performance in a variety of scenarios.
                    
                <br><br>
                    
                <strong>State-of-the-art. </strong>HuBERT is a waveform transformer deep learning model with embedding networks and an attention mechanism developed for speech recognition. Waveform transformer models perform better than convolutional and recurrent neural networks in speech recognition by leveraging self-supervised and supervised pre-training on large speech corpora to learn various phoneme permutations corresponding to complex human speech. In our experimentation, we observed better performance of HuBERT on our recognition tasks compared to a variety of other models.
                    
                <br><br>
                    
                <strong>Model Structure. </strong>Our HuBERT-based classification model consists of three major components: CNN encoder, transformer, and classifier. To adopt the original HuBERT for our purpose, we change the model by (1) modifying the CNN encoder to allow multiple waveform channels, (2) changing the transformer dropout probability, and (3) adding a classification layer to allow HuBERT to be used for spoken digit classification. We implement all of these changes while preserving as much of HuBERT’s pre-training as possible to leverage the benefit of transfer learning. Preserving the pretrained weights is particularly important for the CNN encoder because it helps avoid the vanishing gradient problem that commonly occurs when training deep neural networks. We use the weights of the first layer for each channel of our input signal and change the original dropout probability of 0.1 to 0.05 to better regularize the model for our task. We then designed and added our classifier to process the output of the transformer. The classifier averages the non-masked discovered hidden units and outputs label scores for each classification task. In our classification tasks, gender, digit, and speaker output 1, 10, and 20 scores respectively, which are used to obtain the likelihood of each label and thus the final predicted class.
            </p>
        </div>
        <div style="margin-top: 10%;"> 
            <figure style="width: 100%">
            <center><img width="50%" src="./img/hubert_8ch_class_small.png"></center>
            <div class="overlayTextHalfWide">
                <br>
                    <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 6</strong> The modified HuBERT Classifier model inputs the 8 extracted audio channels into the model. The classifier utilizes the outputted embeddings to infer each sample's label.
                    </font></p>
                </div>
           
        </figure>
        </div>
    </section>
    <section id="Experiments & Results"/></section>
    <section id="Defense"/></section>
</body>
</html>

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>


<head>
    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">google.load("jquery", "1.3.2");</script>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">
      <!-- Custom styles for this template -->
        
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>
    <!-- <link rel="icon" href="img/lightcommands.png"> -->

    <meta name="google-site-verification" content="FeKMujXqA2nHgfvElWNnXzgQWeH9BrG55erv3C2MyiA" />

</head>

<style type="text/css">
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 17px;
        margin-left: auto;
        margin-right: auto;
        width: 70%;
    }

    h1 {
        font-weight: 300;
        line-height: 1.15em;
    }

    h2 {
        font-size: 2em;
    }

    a:link, a:visited {
        color: #00aeff;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }


    b:link, b:visited {
        color: #00aeff;
        text-decoration: none;
    }

    b:hover {
        color: #208799;
    }

    h1, h2{
        text-align: center;
    }

    

    h1 {
        font-size: 40px;
        font-weight: 500;
    }

    h2 {
        font-weight: 400;
        margin: 16px 0px 4px 0px;
    }

    .paper-title {
        padding: 16px 0px 16px 0px;
    }

    section {
        margin: 32px 0px 32px 0px;
        text-align: justify;
        clear: both;
    }

    .col-5 {
        width: 20%;
        float: left;
    }

    .col-4 {
        width: 25%;
        float: left;
    }

    .col-3 {
        width: 33%;
        float: left;
    }

    .col-2 {
        width: 50%;
        float: left;
    }


    .fig-center {
        width: 80%;
        float: center;
    }

    .col-1 {
        width: 100%;
        float: left;
    }

    .row, .author-row, .affil-row {
        overflow: auto;
    }

    .author-row, .affil-row {
        font-size: 26px;
    }

    .row {
        margin: 16px 0px 16px 0px;
    }

    .authors {
        font-size: 26px;
    }

    .affil-row {
        margin-top: 16px;
    }

    .teaser {
        max-width: 100%;
    }

    .text-center {
        text-align: center;
    }

    .fig-caption {
        text-align: center;
        max-width: 100%;

    }

    .audio-caption {
        text-align: center;
        max-width: 100%;

    }

    .screenshot {
        width: 80%;
        border: 1px solid #ddd;
    }

    .screenshot-el {
        margin-bottom: 1px;
    }

    hr {
        height: 1px;
        border: 0;
        border-top: 1px solid #ddd;
        margin: 0;
    }

    .material-icons {
        vertical-align: -6px;
    }

    p {
        line-height: 1.25em;
    }

    .caption {
        font-size: 16px;
        /*font-style: italic;*/
        color: #666;
        text-align: center;
        margin-top: 4px;
        margin-bottom: 10px;
    }

    video {
        display: block;
        margin: auto;
    }

    figure {
        display: block;
        margin: auto;
        margin-top: 10px;
        margin-bottom: 10px;
    }

    #bibtex pre {
        font-size: 13.5px;
        background-color: #eee;
        padding: 16px;
    }

    .blue {
        color: #2c82c9;
        font-weight: bold;
    }

    .orange {
        color: #d35400;
        font-weight: bold;
    }

    .overlayText {
        width: 80%;
        margin-left: auto;
        margin-right: auto;
    }

    .overlayTextFullWide {
        width: 100%;
        margin-left: auto;
        margin-right: auto;
    }

    .overlayTextHalfWide {
        width: 50%;
        margin-left: auto;
        margin-right: auto;
    }

    .flex-row {
        display: flex;
        flex-flow: row wrap;
        justify-content: space-around;
        padding: 0;
        margin: 0;
        list-style: none;
    }
    .table {
	width:100%;
	border:1px solid color-form-highlight;
    }
    
    .table-header {
    	display:flex;
    	width:100%;
    	background:rgb(32, 126, 181);
    	padding:(half-spacing-unit * 1.5) 0;
    }
    
    .table-row {
    	display:flex;
    	width:100%;
    	padding:(half-spacing-unit * 1.5) 0;
    
    }
    
    .table-data, .header__item {
    	flex: 1 1 20%;
    	text-align:center;
    }

    .line{
        border-left-width: 1px;
        border-left-style: solid;
        border-left-color: rgb(92, 84, 84);
    }

    .stack{
        display: block !important;
        width: auto;
    }
    
    .header__item {
    	text-transform:uppercase;
    }

    .paper-btn {
        position: relative;
        text-align: center;

        display: inline-block;
        margin: 8px;
        padding: 8px 8px;

        border-width: 0;
        outline: none;
        border-radius: 2px;

        background-color: #48b64e;
        color: white !important;
        font-size: 20px;
        width: 100px;
        font-weight: 600;
    }

    .paper-btn-parent {
        display: flex;
        justify-content: center;
        margin: 16px 0px;
    }

    .paper-btn:hover {
        opacity: 0.85;
    }

    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
    }
    .boxed {
				padding: 0.5em 2em 2em 2em;
				background-color: #F8F8F8;
	
				max-width: 90%;
				margin: 0 auto !important; 
				float: none !important; ;
	}
	.boxed_mini {
				padding: 0.5em 0.5em 0.5em 0.5em;
				max-width: 40%;
				background-color: #ECF9FF;
				
	}

    .venue {
        /*color: #B6486F;*/
        font-size: 30px;

    }

    .myButton_l {
				display:inline-block;
				cursor:pointer;
				font-family: Montserrat,sans-serif;
				font-weight: bold;
				font-size:15px;
				letter-spacing: 0.1em;
				padding:10px 10px;
				text-decoration:none;
			}
	.myButton {
				 background-color:#87d4f5;
				-moz-border-radius:18px;
				-webkit-border-radius:18px;
				border-radius:18px;
				display:inline-block;
				cursor:pointer;
				color:#ffffff;
				font-family: Montserrat,sans-serif;
				font-weight: bold;
				font-size:25px;
				letter-spacing: 0.1em;
				padding:10px 50px;
				text-decoration:none;
			}
	.myButton:hover {
				background-color:#478fcc;
				color:#ffffff;
				
			} 
            /* Style the navigation menu */
    .navbar {
    /* margin: 0%;
    height: 1%;
    padding: 0%; */
    margin: auto;
    text-align: center;
    padding-top: 0.14%;
    min-height: 25px;
    width: 100%;
    background-color: rgb(0, 140, 255);
    /*text-align: center;*/
    overflow: auto;
    }

    /* Navigation links */
    .navbar a {
    float: left;
    /*height: 90%;*/
    vertical-align: middle;
    color: rgb(255, 255, 255);
    text-decoration: none;
    font-size: 0.8em;
    width: 20%; /* Four equal-width links. If you have two links, use 50%, and 33.33% for three links, etc.. */
    text-align: center; /* If you want the text to be centered */
    }

    /* Add a background color on mouse-over */
    .navbar a:hover {
    background-color: rgb(0, 110, 255);
    }

    /* Style the current/active link */
    .navbar a.active {
    background-color:rgb(0, 120, 255);
    }

    .button-30 {
    align-items: center;
    appearance: none;
    background-color: #FCFCFD;
    border-radius: 4px;
    border-width: 0;
    box-shadow: rgba(45, 35, 66, 0.4) 0 2px 4px,rgba(45, 35, 66, 0.3) 0 7px 13px -3px,#D6D6E7 0 -3px 0 inset;
    box-sizing: border-box;
    color: #36395A;
    cursor: pointer;
    display: inline-flex;
    font-family: "JetBrains Mono",monospace;
    height: 30px;
    justify-content: center;
    line-height: 1;
    list-style: none;
    overflow: hidden;
    padding-left: 10px;
    padding-right: 10px;
    position: relative;
    text-align: left;
    text-decoration: none;
    transition: box-shadow .15s,transform .15s;
    user-select: none;
    -webkit-user-select: none;
    touch-action: manipulation;
    white-space: nowrap;
    will-change: box-shadow,transform;
    font-size: 18px;
    }

    .button-30:focus {
    box-shadow: #D6D6E7 0 0 0 1.5px inset, rgba(45, 35, 66, 0.4) 0 2px 4px, rgba(45, 35, 66, 0.3) 0 7px 13px -3px, #D6D6E7 0 -3px 0 inset;
    }

    .button-30:hover {
    box-shadow: rgba(45, 35, 66, 0.4) 0 4px 8px, rgba(45, 35, 66, 0.3) 0 7px 13px -3px, #D6D6E7 0 -3px 0 inset;
    transform: translateY(-2px);
    }

    .button-30:active {
    box-shadow: #D6D6E7 0 3px 7px inset;
    transform: translateY(2px);
    }


    /* Add responsiveness - on screens less than 500px, make the navigation links appear on top of each other, instead of next to each other */
    @media screen and (max-width: 500px) {
    .navbar a {
        float: none;
        display: block;
        width: 100%;
        text-align: left; /* If you want the text to be left-aligned on small screens */
    }
    }

</style>

<!-- End : Google Analytics Code-->
<script type="text/javascript" src="../js/hidebib.js"></script>
<!-- <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
      rel='stylesheet' type='text/css'> -->
<head>
    <title>Side Eye: Acoustic Eavesdropping from Smartphone Cameras</title>
    <meta property="og:description" content="Side Eye: Acoustic Eavesdropping from Smartphone Cameras"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

</head>

<body>
<div class="flex-row">
    <div class="paper-title">
        <h1 style="color:rgb(71, 132, 216)"><strong>Side Eye: Acoustic Eavesdropping from Smartphone Cameras</strong></h1>
    </div>

    <div id="authors">
        <center>
            <div class="author-row">
                <div class="col-3 text-center"><span style="font-size:25px"><a href="https://longyan97.github.io/">Yan Long</a></span><sup>2</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px"><a href="https://pnaghavi.github.io/">Pirouz Naghavi</a></span><sup>1</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px"><a href="https://www.researchgate.net/profile/Blas-Kojusner">Blas Kojusner</a></span><sup>1</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px"><a href="https://www.cise.ufl.edu/~butler/">Kevin Butler</a></span><sup>1</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px"><a href="https://sararampazzi.com/">Sara Rampazzi</a></span><sup>1</sup></div>
                <div class="col-3 text-center"><span style="font-size:25px"><a href="https://web.eecs.umich.edu/~kevinfu/">Kevin Fu</a></span><sup>2</sup></div>
            </div>
            <br>
            <div class="text-left mt-auto" style="width:100%;margin-top: 1.5em; margin-bottom: 1.5em; display:flex;justify-content:space-around;align-items:center; flex-wrap: wrap;">
                <div>
                <a href="https://www.eng.ufl.edu/">
                  <img width = "320px" src="./img/uflogo.PNG" alt="University of Florida logo" class="img-fluid" />
              <div style = "height:10px"></div>
                </a>
                </div>
                <div>
                    <a href="https://www.cse.umich.edu">
                      <img width = "350px" src="./img/University-of-Michigan-Logo.png" alt="University of Michigan logo" class="img-fluid" />
                  <div style = "height:10px"></div>
                    </a>
                    </div>
                </div>
            <!-- <center>
                <br><br>
                <table align=center width=800px>
                    <tr>
                        <td align=center width=300px>
                            <center>
                                <span style="font-size:15px"><sup>1</sup> University of Florida</span>
                            </center>

                        </td>
                        <td align=center width=300px>
                            <center>
                                <span style="font-size:15px"><sup>2</sup> University of Michigan</span>
                            </center>
                        </td>
                        <td align=center width=300px>
                            <center>
                                <span style="font-size:15px"><sup>3</sup> The University of Electro-Communications (Tokyo)</span>
                            </center>
                        </td>
                    </tr>
                </table>
            </center>

        </center> -->
        
    </div></div>
    <!-- The navigation menu -->
    <div class="navbar" style="height: 5px;">
        <a class="active" href="#Overview">Overview</a>
        <a href="#Recovered Signals">Audio Samples</a>
        <a href="#Processing Pipeline and ML Setup">Signal Proc & Deep Learning</a>
        <a href="#Vulnerability Causality">Vulnerability Causality</a>
        <a href="#Q&A">Q&A</a>
    </div>

    <section id="Overview"/>
    <hr>
    <div class="flex-row">
        <p>
            <br>
            <strong>Overview:</strong> 
            <!-- Our research discovers how the rolling shutter and movable lens structures widely found in smartphone cameras modulate structure-borne sounds onto camera images, creating a point-of-view (POV) optical-acoustic side channel for acoustic eavesdropping. The movement of smartphone camera hardware leaks acoustic information because images unwittingly modulate ambient sound as imperceptible distortions. Our experiments find that the side channel is further amplified by intrinsic behaviors of Complementary metal-oxide–semiconductor (CMOS) rolling shutters and movable lenses such as in Optical Image Stabilization (OIS) and Auto Focus (AF). Our paper characterizes the limits of acoustic information leakage caused by structure-borne sound that perturbs the POV of smartphone cameras. In contrast with traditional optical-acoustic eavesdropping on vibrating objects, this side channel requires no line of sight and no object within the camera's field of view (images of a ceiling suffice). Our experiments test the limits of this side channel with a novel signal processing pipeline that extracts and recognizes the leaked acoustic information. Our evaluation with 10 smartphones on a spoken digit dataset reports 80.66%, 91.28% and 99.67% accuracies on recognizing 10 spoken digits, 20 speakers, and 2 genders respectively. We further systematically discuss the possible defense strategies and implementations. By modeling, measuring, and demonstrating the limits of acoustic eavesdropping from smartphone camera image streams, our contributions explain the physics-based causality and possible ways to reduce the threat on current and future devices. -->
            Side Eye is a vulnerability in smartphone camera hardware that allows adversaries to extract acoustic information from a stream of camera photos. 
            When sound energy vibrates the smartphone body, it shakes camera lenses and creates tiny Point-of-View (POV) variations in the camera images.
            Adversaries can thus infer the sound information by analyzing the image POV variations. 
            Side Eye enables a family of sensor-based side channel attacks targeting acoustic eavesdropping even when smartphone microphone is disabled, 
            and does not require specific objects within the camera's field of view.  
            
            Side Eye is to appear at <a href="https://www.ieee-security.org/TC/SP2023/">IEEE Symposium on Security and Privacy 2023</a> <br><br>

            Code and replication: Side Eye is a patent-pending technology so we are unable to share the detailed system implementation now. If you have questions regarding how to implement it, feel free to contact <a href="https://longyan97.github.io/">Yan Long</a>. In addition, we provide the <a href="https://github.com/longyan97/VID-IMU-Logger/tree/main">Android app for video recording</a>. 


        </p>
    </div>

	<div> 
        <figure style="width: 80%">
        <center><img width="80%" src="./img/proj_setup.png"></center>
		<div class="overlayText">
                <br>
                <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 1:</strong> 
                    Illustration of using  Side Eye to extract acoustic information produced by nearby electronic speakers. 
                    The POV-based side channel does not require any specific objects within the camera's field of view. Images of the ceiling or floor in the room suffice. </font></p>
            </div>
       
        </figure>
    </div>			
	<center>	

	<a href="https://arxiv.org/abs/2301.10056" class="myButton">Read the Paper</a>

    
	<a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Side+Eye%3A+Characterizing+the+Limits+of+POV+Acoustic+Eavesdropping+from+Smartphone+Cameras+with+Rolling+Shutters+and+Movable+Lenses&btnG=" class="myButton">Cite</a>					
			
			
		<!-- <a href="#bibtex"  class=" myButton " data-toggle="collapse" role="button"><span class="material-icons"> insert_comment </span>
	Cite <i class="fa fa-quote-right" aria-hidden="true"></i></a>
		<div id="bibtex" style="margin-top: 1.5em;" class="collapse" align="left">
			<pre style="white-space: pre">

                @INPROCEEDINGS {,
                    author = {Y. Long and P. Naghavi and B. Kojusner and K. Butler and S. Rampazzi and K. Fu},
                    booktitle = {2023 2023 IEEE Symposium on Security and Privacy (SP) (SP)},
                    title = {Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses},
                    year = {2023},
                    volume = {},
                    issn = {},
                    pages = {1032-1049},
                    abstract = {Our research discovers how the rolling shutter and movable lens structures widely found in smartphone cameras modulate structure-borne sounds onto camera images, creating a point-of-view (POV) optical-acoustic side channel for acoustic eavesdropping. The movement of smartphone camera hardware leaks acoustic information because images unwittingly modulate ambient sound as imperceptible distortions. Our experiments find that the side channel is further amplified by intrinsic behaviors of Complementary metal-oxide&amp;#x2013;semiconductor (CMOS) rolling shutters and movable lenses such as in Optical Image Stabilization (OIS) and Auto Focus (AF). Our paper characterizes the limits of acoustic information leakage caused by structure-borne sound that perturbs the POV of smartphone cameras. In contrast with traditional optical-acoustic eavesdropping on vibrating objects, this side channel requires no line of sight and no object within the camera&amp;#x27;s field of view (images of a ceiling suffice). Our experiments test the limits of this side channel with a novel signal processing pipeline that extracts and recognizes the leaked acoustic information. Our evaluation with 10 smartphones on a spoken digit dataset reports 80.66%, 91.28%, and 99.67% accuracies on recognizing 10 spoken digits, 20 speakers, and 2 genders respectively. We further systematically discuss the possible defense strategies and implementations. By modeling, measuring, and demonstrating the limits of acoustic eavesdropping from smartphone camera image streams, our contributions explain the physics-based causality and possible ways to reduce the threat on current and future devices.},
                    keywords = {},
                    doi = {10.1109/SP46215.2023.00059},
                    url = {https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.00059},
                    publisher = {IEEE Computer Society},
                    address = {Los Alamitos, CA, USA},
                    month = {may}
                    }                 

    
	</pre>					
	</div>	 -->

    <a href="https://dl.acm.org/doi/abs/10.1145/3584318.3584321" class="myButton">Related</a>      

    </center>
    

    </section>

    <br>


    <div style="width:1125px;" >
        <div style="float:left;">
          <iframe width="560px" height="315px" src="https://www.youtube.com/embed/zYT-q4dQglU" frameborder="0" allowfullscreen></iframe>
        </div>
        <div style="float:right;">
      <iframe width="560px" height="315px" src="https://www.youtube.com/embed/W7ljxXxOem0" frameborder="0" allowfullscreen></iframe>
        </div>
        <div style="clear:both;"></div>
      </div>


    <section id="Recovered Signals"/>
    <h2>Audio Samples</h2>
    <hr>
    <div class="flex-row">
        <p>
            <br>Some examples of the original and side channel-recovered audio signals are provided below. 

            The original audio is from the <a href="https://github.com/soerenab/AudioMNIST">AudioMNIST dataset</a>. 

            The recovered signals are extracted from image streams recorded by the rear camera of a Google Pixel 3 smartphone when 
            an electronic speaker is playing the original audio at 85 dB. 
            <br><br>
            Although the recovered audio does not allow listeners to directly tell what exact words are spoken without any reference, 
            it preserves clear low-frequency tones that allow listeners to tell the gender and even identity of the speaker. 
            Listeners may also be able to differentiate between different words and sentences when given a closed set of possible candidates.

            Side Eye further provides <a href="#Processing Pipeline and ML Setup">a deep learning model</a> that can recognize individual 
            words with high accuracies. 

        </p>
    </div>

    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/36.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText"style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Original audio of speaker 36 (Female)</p>
            </div>
    </div>
    
    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/36_recover.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Recovered audio of speaker 36 (Female)</p>
            </div>
    </div>

    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/56.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Original audio of speaker 56 (Female)</p>
            </div>
    </div>
    
    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/56_recover.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Recovered audio of speaker 56 (Female)</p>
            </div>
    </div>

    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/06.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText"style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Original audio of speaker 6 (Male)</p>
            </div>
    </div>
    
    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/06_recover.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Recovered audio of speaker 6 (Male)</p>
            </div>
    </div>

    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/09.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Original audio of speaker 9 (Male)</p>
            </div>
    </div>
    
    <div class="col-2 text-center" style="margin-top: 1%;"> 
        <center><audio controls autoplay muted><source src="./audio/09_recover.wav" type="audio/mpeg"></audio></center>
		<div class="overlayText" style="margin-top: 0.9%;">

                <p id="topText" style="margin-top: 0.9%;" class="audio-caption">Recovered audio of speaker 9 (Male)</p>
            </div>
    </div>



    <div style="margin-top: 40%;"> 
        <figure style="width: 100%">
        <center><img width="80%" src="./img/digit.png"></center>
		<div class="overlayText">
            <br>
                <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 2:</strong> The waveform and spectrogram of spoken digits "zero", "seven", and "nine". 
                    (Left) The original signals. (Right) The recovered signals from a 3.2s 30fps video with the optimized camera parameters.
                </font></p>
            </div>
       
    </figure>
    </div>
	</section>

    <section id="Processing Pipeline and ML Setup"/>
		<br>
        <h2>Signal Processing and Deep Learning</h2>
        <hr>
        <div class="flex-row">
            <br>
            <p>
                
                Side Eye extracts the POV variations in image streams using a diffusion image registration-based algorithm. 
                The extraction algorithm exploits <a href="#Processing Pipeline and ML Setup">the rolling shutter feature of smartphone cameras</a> to
                increase the effective sample rates of recoverable acoustic signals from the image stream frame rate (~30Hz)
                to the camera's rolling shutter frequency (over 30k Hz). 
                The extracted raw signals are further cleaned up by a pre-processing pipeline, whose output are demonstrated by the <a href="#Recovered Signals">audio samples</a>. 
                <br><br>
                Side Eye  provides a HuBERT-based deep learning model that can recognize the gender (2-class), identity (20-class), 
                and speaker independent spoken digits (10-class) with <strong>up to  99.67%, 91.28%, and 80.66% accuracies. </strong> 
                <br><br>
                Our evaluations show that the signal processing and deep learning pipeline allow adversaries to extract <strong>non-trivial acoustic information in various scenarios</strong>, including: (1) the speaker and phone are left on different desks; 
                (2) the speaker is left on the floor and the phone is in the pocket of a shirt or a backpack worn by a mannequin; 
                (3) the speaker and phone are placed in two different rooms.
    
            </p>
        </div>


        <div style="margin-top: 1%;"> 
            <figure style="width: 100%">
            <center><img width="100%" src="./img/pipeline_small.png"></center>
            <div class="overlayTextFullWide">
                <br>
                    <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 3:</strong> Side Eye's signal processing and deep learning pipeline.
                    </font></p>
                </div>
           
        </figure>
        </div>



        <div style="margin-top: 5%;"> 
            <figure style="width: 100%">
            <center><img width="100%" src="./img/scenarios.png"></center>
            <div class="overlayTextFullWide">
                <br>
                    <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 4:</strong> The optical-acoustic side channel of Side Eye allows adversaries to eavesdrop on acoustic information in various scenarios.
                    </font></p>
                </div>
           
        </figure>
        </div>


<!--         
        <div class="flex-row">
            <br>
            <p>
                <strong>Deep Learning Model: </strong> <br>

                We 

                <br><br>
                <strong>Transfer Learning. </strong>HuBERT is a machine learning model that implements the concept of transfer learning. Essentially, the acoustic signals we extract are the embeddings of the original speech signals in the lens motion domain. Although building classification models in such a new signal domain often requires a significant amount of data to sufficiently sample the target domain, the adversary can overcome this challenge by utilizing transfer learning. Simply put, transfer learning allows the model to leverage the knowledge gained in the source domain (speech audio) and apply it to the tasks in the target domain (lens motion). Specifically, the original HuBERT model was pre-trained on 60,000 hours of speech derived from the LibriVox project and formalized in Libri-light, as well as 960 hours of speech from Librispeech. This large amount of incorporated knowledge enables the model to rapidly adapt to the lens motion distribution.
                    
                <br><br>
                    
                <strong>Waveform Input. </strong>HuBERT uses raw waveform as inputs. Compared to other popular types of speech recognition inputs, such as mel-frequency cepstral coefficients (MFCCs) and signal spectrogram, raw waveform inputs preserve the largest amount of usable information. Raw audio waveform inputs have also been demonstrated in speech recognition research to achieve higher performance in a variety of scenarios.
                    
                <br><br>
                    
                <strong>State-of-the-art. </strong>HuBERT is a waveform transformer deep learning model with embedding networks and an attention mechanism developed for speech recognition. Waveform transformer models perform better than convolutional and recurrent neural networks in speech recognition by leveraging self-supervised and supervised pre-training on large speech corpora to learn various phoneme permutations corresponding to complex human speech. In our experimentation, we observed better performance of HuBERT on our recognition tasks compared to a variety of other models.
                    
                <br><br>
                    
                <strong>Model Structure. </strong>Our HuBERT-based classification model consists of three major components: CNN encoder, transformer, and classifier. To adopt the original HuBERT for our purpose, we change the model by (1) modifying the CNN encoder to allow multiple waveform channels, (2) changing the transformer dropout probability, and (3) adding a classification layer to allow HuBERT to be used for spoken digit classification. We implement all of these changes while preserving as much of HuBERT’s pre-training as possible to leverage the benefit of transfer learning. Preserving the pretrained weights is particularly important for the CNN encoder because it helps avoid the vanishing gradient problem that commonly occurs when training deep neural networks. We use the weights of the first layer for each channel of our input signal and change the original dropout probability of 0.1 to 0.05 to better regularize the model for our task. We then designed and added our classifier to process the output of the transformer. The classifier averages the non-masked discovered hidden units and outputs label scores for each classification task. In our classification tasks, gender, digit, and speaker output 1, 10, and 20 scores respectively, which are used to obtain the likelihood of each label and thus the final predicted class.
            </p>
        </div>
        <div style="margin-top: 10%;"> 
            <figure style="width: 100%">
            <center><img width="50%" src="./img/hubert_8ch_class_small.png"></center>
            <div class="overlayTextHalfWide">
                <br>
                    <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 6</strong> The modified HuBERT Classifier model inputs the 8 extracted audio channels into the model. The classifier utilizes the outputted embeddings to infer each sample's label.
                    </font></p>
                </div>
           
        </figure>
        </div> -->
    </section>





<section id="Vulnerability Causality"/>



<h2>Vulnerability Causality</h2>
    <hr>
    <div class="flex-row">
        <br>
        <p>
            
            The culprits of Side Eye attacks are the <strong>rolling shutter architectures</strong>  and  <strong>moveable lens structures</strong>  found in most smartphone cameras. <br><br>
            
            Rolling shutters expose each row of the camera image sensor sequentially. This increases the number of sampling points that adversaries have access to when performing computations on the image streams, increasing the amount of acoustic information leaked into images. Without exploiting rolling shutters, the sample rate of the acoustic signals that the adversaries can recover is the same as the frame rate of the image streams (often 30 Hz). By understanding and exploiting rolling shutters, the adversaries can increase the sample rate to the same as the rolling shutter frequency, i.e., the row-wise sequential scanning frequency (often over 30k Hz). <br> <br>

            Moveable lenses amplify the camera's optical path variations when the phones are vibrated by sound energy. There exists relative movements between the camera lens and smartphone body when the smartphone body moves under sound waves. This increases the amplitude of the sound-induced POV variations in the images, leading to significantly higher signal-to-noise ratios for the adversaries. These moveable lens structures are employed by camera manufacturers for auto-focusing (AF) and optical image stabilization (OIS) functions. 

        </p>
    </div>



    <div style="margin-top: 5%;"> 
        <figure style="width: 100%">
        <center><img width="100%" src="./img/RSOIS.png"></center>
        <div class="overlayTextFullWide">
            <br>
                <p id="topText" class="fig-caption"><font size="-1"><strong>Fig. 5:</strong> (Left) Rolling shutter architectures increase the effective sample rate of the audio signals extracted that can be extracted by the adversaries from image streams. (Right) Moveable len structures amplify the image variation caused by sound-induced smartphone body vibrations. 
                </font></p>
            </div>
       
    </figure>
    </div>





</section>





<section id="Q&A"/>
<h2>Frequently Asked Questions</h2>
<hr>


<h3>General Questions</h3>



<div class="card">
    <div id="q-name">
        <h4>
            
            <button class="button-30" data-toggle="collapse" data-target="#a-name" >
                Why is it called Side Eye? 
            </button>
        </h4>
    </div>

    <div id="a-name" class="collapse">
        <div class="card-body">
            <p> The camera senses nearby sounds without actually aiming at any objects generating or being vibrated by the sound waves because the sound waves vibrate the camera lens itself and cause point-of-view variations. We call this attack Side Eye because the camera is "seeing" something aside that is out of its field-of-view.  </p>
        </div>
    </div>
</div>



<div class="card">
    <div id="q-devices">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-devices" >
                What devices are affected by  Side Eye? 
            </button>
        </h4>
    </div>

    <div id="a-devices" class="collapse">
        <div class="card-body">
            <p>Almost all modern smartphone cameras since rolling shutters and moveable lenses are ubiquitous nowadays. The rear (main) cameras are often more susceptible because their lenses can move more under ambient sound waves. In <a href="TBA">our research paper</a>, we demonstrated case studies on 10 smartphone models from Samsung, Google, and Apple.  </p>
        </div>
    </div>
</div>


<div class="card">
    <div id="q-threats">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-threats" >
                When do I face such threats?
            </button>
        </h4>
    </div>

    <div id="a-threats" class="collapse">
        <div class="card-body">
            <p> Side Eye attacks can happen if adversaries get access to a burst of camera photos (in another word, a muted video) from your smartphone when the smartphone camera recorded the photos near an electronic speaker playing audio. Such electronic speakers can be  standalone loudspeakers in the room or even the internal speakers of your TVs, laptops, and smartphones. The adversaries can be either passive eavesdroppers to whom you share your photos without knowing the photos could contain audio information, or active attackers  taking the form of smartphone applications that are granted permissions to use smartphone cameras.  </p>
        </div>
    </div>

</div>


<div class="card">
    <div id="q-info">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-info" >
                What type of acoustic information can be leaked through smartphone images?
            </button>
        </h4>
    </div>


    <div id="a-info" class="collapse">
        <div class="card-body">
            <p> Almost all types of audio information (e.g., speech, music, etc.) from electronic speakers can be leaked with present-day smartphone cameras. Louder audio can be leaked more easily because it can induce larger POV variations in images. That said, we found audio played at normal and quiet conversation volumes (65 and 55 dB SPL) can also leak a significant amount of information when the acoustic signals are extracted and recognized by our signal processing and deep learning pipeline. 
                <br><br>
                
            It is worth noting that our evaluations show Side Eye cannot be used to eavesdrop on audio from human speakers with present-day smartphone cameras yet because sound waves generated by human speakers cannot vibrate the lenses enough to generated distinguishable POV variations. Nevertheless, future cameras with higher pixel resolution and lower imaging noises might be able to capture human speech signals. 
                
            </p>
        </div>
    </div>


</div>




<div class="card">
    <div id="q-avoid">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-avoid" >
                How can I avoid Side Eye attacks?
            </button>
        </h4>
    </div>


    <div id="a-avoid" class="collapse">
        <div class="card-body">

            <p> For smartphone users, the first thing is to realize that cameras can also leak audio. So when granting camera access to applications, keep in mind you could also be sharing some audio information to the applications even if you do not give them microphone access. Furthermore, you may want to be cautious when sharing muted videos to others, especially if they were taken near electronic speakers. You may also consider putting your cameras away from electronic speakers in the room to reduce the amount of audio leaked into camera images. A useful tip for assessing if there is a risk of Side Eye attack is to  put your hand on your smartphone and feel if there is sound-induced vibrations. If your hand feel vibrations caused by nearby electronic speakers, then there is a high risk your camera can also capture it.  </p>

        </div>
    </div>


</div>



<div class="card">
    <div id="q-camfix">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-camfix" >
                Are there any fixes to cameras that can address this problem?
            </button>
        </h4>
    </div>


    <div id="a-camfix" class="collapse">
        <div class="card-body">
            <p> <a href="TBA">Our research paper</a> discussed in detail the possible hardware fixes that camera manufacturers may consider, including those aiming to address the rolling shutter problem and those aiming to address the movable lens problem respectively. We found preventing camera lenses from moving when it is not supposed to (i.e., when there is ambient sounds) is the most effective solution. From the user standpoint, this can be achieved by placing external magnets by the smartphone cameras to fix the position of the internal lenses whose supporting structures are also magnet-based. This is similar to how people tried to use magnets to <a href="https://www.youtube.com/watch?v=NuCA6lsFjRs">fix their shaking camera problems</a>. However, this temporary fix carried out by users may damage the cameras' internal mechanical and electrical structures.   
            <br><br>

            It is also worth pointing out that we haven't found any effective software fixes to this problem since the root cause is hardware-based. Software approaches such as using lower resolution images will greatly affect usability while only providing very limited decrease in the acoustic information captured by cameras. 
            
            </p>
            

        </div>
    </div>


</div>









<h3>More Technical Questions</h3>



<div class="card">
    <div id="q-otherapp">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-otherapp" >
                What other applications may be supported by the technique of Side Eye?
            </button>
        </h4>
    </div>


    <div id="a-otherapp" class="collapse">
        <div class="card-body">

            <p> The techniques used by Side Eye provides a mechanism for extracting other vibrational modalities of signals (e.g., acceleration and angular velocity) from images, potentially reducing the number of sensors needed. Besides adversarial applications, it may also be used by system defenders. Our workshop paper <a href="https://arxiv.org/abs/2301.11745">Side Auth</a> provides some insights and examples.  </p>

        </div>
    </div>


</div>




<div class="card">
    <div id="q-global">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-global" >
                Will a global shutter prevents Side Eye attacks completely?
            </button>
        </h4>
    </div>


    <div id="a-global" class="collapse">
        <div class="card-body">

            <p> Unfortunately, it cannot. With a global shutter, the number of sampling points decreases by 1080x with 1080p-resolution images. But Side Eye's deep learning model is still able to exploit the few sampling points (with a sample rate same as the frame rate) to recover non-trivial information. For example, we found out the accuracies of recognizing 10-class spoken digits drop from ~80% to ~40% when the rolling shutter is replaced with a global shutter. Although rolling shutters capture additional inner-frame information, global shutters still capture inter-frame information. </p>

        </div>
    </div>


</div>







<div class="card">
    <div id="q-diff_motion">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-diff_motion" >
                How does Side Eye relate to previous acoustic eavesdropping attacks using smartphone motion sensors?
            </button>
        </h4>
    </div>


    <div id="a-diff_motion" class="collapse">
        <div class="card-body">

            <p> 

                First, smartphone motion sensors measure smartphone vibrations caused by sound energy directly. Side Eye decodes such signals from videos based on understandings of how sound waves are modulated onto POV variations of camera images. 
                <br><br>
                
                Second, most smartphone motion sensors have maximum sample rate of about 500 Hz, allowing adversaries to infer acoustic signals with a bandwidth of up to 250 Hz. Side Eye exploits the rolling shutter features of cameras and support over 30k Hz sample rate. We observe a signal bandwidth of up to 600 Hz so far. 
                <br><br>
                
                Third, motion sensor data is rarely shared over internet. Side Eye studies another medium of information: camera images/muted videos, which are often shared by users themselves at will to others. This creates an orthogonal space of possible information leakage scenarios.  
            </p>

        </div>
    </div>


</div>





<div class="card">
    <div id="q-diff_cam">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-diff_cam" >
                How does Side Eye relate to previous research of audio measurement using cameras?
            </button>
        </h4>
    </div>


    <div id="a-diff_cam" class="collapse">
        <div class="card-body">

            <p> 
                Previous research of using cameras to measure audio signals requires aiming specialized high-quality cameras at objects that are either generating sounds (e.g., loudspeakers and humans) or vibrated by sound (e.g., lightweight chip bags). In short, they require specific objects in the camera field-of-view. Side Eye exploits the point-of-view variations caused by smartphone camera intrinsic physics. Furthermore, Side Eye provide signal extraction algorithms that are optimized for  extracting POV variations and a state-of-the-art deep learning model for recognizing the extracted signals which are difficult for humans to understand directly. 
                
            </p>

        </div>
    </div>


</div>


<div class="card">
    <div id="q-diff_model">
        <h4>
            <button class="button-30" data-toggle="collapse" data-target="#a-diff_model" >
                Can Side Eye only be carried out using the HuBERT large classifier model described in the paper?
            </button>
        </h4>
    </div>


    <div id="a-diff_model" class="collapse">
        <div class="card-body">

            <p> 
                No, we were able to obtain the best results with the HuBERT classifier model, but we had relatively similar success with much smaller models, such as ResNET-50 using spectrograms as inputs. When using different models for classification, improved performance is obtained when the model has been pre-trained on other and perhaps unrelated datasets such as ImageNet. Also, our experiments with different models indicate that using 8 extracted channels from the optical-acoustic side-channel outperforms using only one channel. As a result, the classification model may need to be modified to accept additional input channels.
                
            </p>

        </div>
    </div>


</div>























</section>




</body>
</html>
